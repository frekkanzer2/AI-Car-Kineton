Steps,Policy/Entropy,Policy/Extrinsic Value Estimate,Environment/Episode Length,Environment/Cumulative Reward,Policy/Extrinsic Reward,Is Training
10000,1.4117982,-522.47485,302.64285714285717,-37069.27777777778,-37069.27777777778,1.0
20000,1.4134568,-534.32367,328.6666666666667,-41941.27419354839,-41941.27419354839,1.0
30000,1.4122001,-548.27515,338.3103448275862,-43476.91379310345,-43476.91379310345,1.0
40000,1.4130428,-543.13605,358.41379310344826,-40120.68965517241,-40120.68965517241,1.0
50000,1.4153953,-565.62244,317.80645161290323,-40320.62903225807,-40320.62903225807,1.0
60000,1.415674,-556.18054,351.8,-37808.63333333333,-37808.63333333333,1.0
70000,1.4160997,-566.7864,372.1304347826087,-41436.608695652176,-41436.608695652176,1.0
80000,1.4171181,-576.7948,465.09090909090907,-41971.0,-41971.0,1.0
90000,1.4183444,-570.9115,504.05555555555554,-36188.5,-36188.5,1.0
100000,1.4181255,-596.8541,489.9047619047619,-49542.52380952381,-49542.52380952381,1.0
110000,1.4189847,-592.3446,531.7368421052631,-38842.944444444445,-38842.944444444445,1.0
120000,1.4202285,-595.6624,514.9444444444445,-41638.57894736842,-41638.57894736842,1.0
130000,1.4206238,-601.87964,528.6190476190476,-32988.166666666664,-32988.166666666664,1.0
140000,1.421266,-626.7499,474.6842105263158,-51715.47368421053,-51715.47368421053,1.0
