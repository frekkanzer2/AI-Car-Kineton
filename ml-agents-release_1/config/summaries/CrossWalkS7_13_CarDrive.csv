Steps,Policy/Entropy,Policy/Extrinsic Value Estimate,Environment/Episode Length,Environment/Cumulative Reward,Policy/Extrinsic Reward,Is Training
390000,0.8163947,129.82524,169.0,266.74135335286456,266.74135335286456,1.0
400000,0.81637454,227.59657,286.7142857142857,6386.1126757727725,6386.1126757727725,1.0
410000,0.8160126,305.68732,481.7368421052632,24989.716636657715,24989.716636657715,1.0
420000,0.81583077,189.88722,340.86206896551727,6498.562629962789,6498.562629962789,1.0
430000,0.81571096,344.79135,468.8695652173913,21869.583072164784,21869.583072164784,1.0
440000,0.8156076,348.3053,354.9583333333333,18368.453927675884,18368.453927675884,1.0
450000,0.8156437,252.27617,382.46666666666664,6909.074173227946,6909.074173227946,1.0
460000,0.8157048,246.50227,433.6190476190476,22259.633626302082,22259.633626302082,1.0
470000,0.8156274,272.4912,347.06666666666666,11479.024700822501,11479.024700822501,1.0
480000,0.8156239,337.96176,421.6190476190476,11029.958547245373,11029.958547245373,1.0
490000,0.8156187,279.09726,515.8125,28809.62774348259,28809.62774348259,1.0
500000,0.81561685,250.98671,814.9333333333333,41288.13347409566,41288.13347409566,1.0
