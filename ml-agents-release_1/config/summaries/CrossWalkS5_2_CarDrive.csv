Steps,Policy/Entropy,Policy/Extrinsic Value Estimate,Environment/Episode Length,Environment/Cumulative Reward,Policy/Extrinsic Reward,Is Training
1370000,1.4119567,-502.0425,310.90909090909093,-29846.761904761905,-29846.761904761905,1.0
1380000,1.4120429,-506.82568,354.52,-26748.423076923078,-26748.423076923078,1.0
1390000,1.4119965,-509.58603,325.78125,-40117.46875,-40117.46875,1.0
1400000,1.4118457,-494.5633,320.54545454545456,-24246.560606060608,-24246.560606060608,1.0
1410000,1.411755,-515.14734,286.19354838709677,-39777.06451612903,-39777.06451612903,1.0
1420000,1.4116791,-510.3562,340.06666666666666,-36020.6724137931,-36020.6724137931,1.0
1430000,1.4116462,-514.9687,318.83870967741933,-37803.078125,-37803.078125,1.0
1440000,1.4117054,-513.4457,359.42857142857144,-37761.78571428572,-37761.78571428572,1.0
1450000,1.4117708,-514.44635,337.42857142857144,-35318.55357142857,-35318.55357142857,1.0
1460000,1.4118036,-513.54364,352.7241379310345,-41179.65517241379,-41179.65517241379,1.0
1470000,1.4118031,-505.764,341.2413793103448,-30482.706896551725,-30482.706896551725,1.0
1480000,1.4117815,-519.9423,363.9,-45178.666666666664,-45178.666666666664,1.0
1490000,1.4117864,-517.4262,334.37931034482756,-34979.637931034486,-34979.637931034486,1.0
1500000,1.4117982,-522.1861,344.85185185185185,-38146.68518518518,-38146.68518518518,1.0
